{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bf7aaf-4fb8-4250-8ae9-16db5cf7fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement 1:\n",
    "    # Read from .xls\n",
    "    # Write to .xls\n",
    "\n",
    "# Excel Python codecs: (Read/Write)\n",
    "    # xlrd, xlwt : For xls format decoding/encoding\n",
    "    # calamine : Rust implementation, binded to Python for xls, xlsx, xlsm, xlsb, xla, xlam decoding\n",
    "    # openpyxl : For '.xlsx', '.xlsm', '.xltx', '.xltm' decoding/encoding\n",
    "    # pandas: Library providing high-level API based on above 3(and more) libraries.\n",
    "\n",
    "# Apache Spark \"DataSource\" compatible codecs:\n",
    "    # Crealytics Spark Excel - \"com.crealytics:spark-excel_2.12:3.5.1_0.20.4\"\n",
    "\n",
    "# Most libraries interpret .xlsx as \"flat\" data, \n",
    "# without support for operations/functions and derived cells. (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4efd0b-5933-47d3-9000-a2dce1f02c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement 2:\n",
    "  # Call api\n",
    "\n",
    "# HTTP REST API clients:\n",
    "  # requests: HTTP REST library (Use with Spark as a Python UDF?)\n",
    "\n",
    "# Apache Spark \"DataSource\" compatible HTTP REST API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931937d6-c515-4a55-8496-c3ec272f2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement 3:\n",
    "    # Do debugging\n",
    "    # Fix errors\n",
    "    # Record error handling\n",
    "\n",
    "# Logging in Python:\n",
    "    # logging library\n",
    "    # print() statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43fa65e-ba00-473b-a0b9-b27d64e8a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement 4:\n",
    "    # CRUDE Operations add, edit delete with database, python, web UI\n",
    "\n",
    "# Unsure how to implement HTTP servers in distributed manner.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d3fa29-1f9e-4054-a3b1-222c9ab29a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf9ba210-5cf4-49c7-b597-fde735921bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MongoDB connection configuration\n",
    "secrets = {\n",
    "  'mongodb':{\n",
    "    'connectionString': \"localhost:1001\"\n",
    "  }\n",
    "}\n",
    "connectionString = secrets['mongodb']['connectionString']\n",
    "\n",
    "# Catalyst optimizer extensions for connectors:\n",
    "extensionClasses = [\n",
    "]\n",
    "\n",
    "mvnPackages = [\n",
    "  \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\", # MongoDB-Spark connector\n",
    "  \"com.crealytics:spark-excel_2.12:3.5.1_0.20.4\", # Excel file format decoder\n",
    "]\n",
    "\n",
    "# Read options for Spark DataFrameReader Excel Format:\n",
    "\n",
    "excelReadOpts = {\n",
    "  \n",
    "  # 'dataAddress': \"A1\", # Optional, default: \"A1\"\n",
    "\n",
    "  'header': 'true', # Required\n",
    "  'treatEmptyValuesAsNulls': 'false', # Optional, default: true\n",
    "\n",
    "  # # Optional, default: false, where errors will be converted to null. \n",
    "  # # If true, any ERROR cell values (e.g. #N/A) will be converted to \n",
    "  # # the zero values of the column's data type.\n",
    "  # 'setErrorCellsToFallbackValues': 'true',\n",
    "  \n",
    "  # Optional, default: false, If true, format the cells \n",
    "  # without rounding and scientific notations\n",
    "  'usePlainNumberFormat': 'false',\n",
    "  \n",
    "  'inferSchema': 'false', # Optional, default: false\n",
    "  'addColorColumns': 'true', # Optional, default: false\n",
    "\n",
    "  # Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]\n",
    "  'timestampFormat': 'MM-dd-yyyy HH:mm:ss', \n",
    "\n",
    "  # # Optional, default None. If set, uses a streaming reader \n",
    "  # # which can help with big files (will fail if used with .xls format files)\n",
    "  # 'maxRowsInMemory': 20,\n",
    "\n",
    "  # # Optional, default None. \n",
    "  # # See https://poi.apache.org/apidocs/5.0/org/apache/poi/util/IOUtils.html#setByteArrayMaxOverride-int-\n",
    "  # 'maxByteArraySize': 2147483647,\n",
    "\n",
    "  # Optional, default None. Number of bytes at which a \n",
    "  # zip entry is regarded as too large for holding in \n",
    "  # memory and the data is put in a temp file instead\n",
    "  'tempFileThreshold': 10000000,\n",
    "\n",
    "  # Optional, default: 10. If set and if schema inferred, \n",
    "  # number of rows to infer schema from\n",
    "  'excerptSize': 10,\n",
    "  \n",
    "  # # Optional, default None. \n",
    "  # # Requires unlimited strength JCE for older JVMs\n",
    "  # 'workbookPassword': 'pass'\n",
    "}\n",
    "\n",
    "# It does not allow to add packages built as \"Wheels\" and \n",
    "# therefore does not allow to include dependencies with native code\n",
    "\n",
    "# .zip Python packages ('egg') or .py file dependencies\n",
    "pyPackagePaths = [\n",
    "  \"\", # For making REST API calls\n",
    "]\n",
    "\n",
    "# Spark Session configuration:\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .master('local[8]') \\\n",
    "  .appName(\"API->DB\") \\\n",
    "  .config(\"spark.executor.memory\", \"4g\") \\\n",
    "  .config(\"spark.driver.memory\", \"8g\") \\\n",
    "  .config(\"spark.jars.packages\",','.join(mvnPackages)) \\\n",
    "  .config(\"spark.sql.extensions\", ','.join(extensionClasses)) \\\n",
    "  .config(\"spark.mongodb.read.connection.uri\",connectionString) \\\n",
    "  .config(\"spark.mongodb.read.database\",\"api\") \\\n",
    "  .config(\"spark.mongodb.write.database\",\"api\") \\\n",
    "  .config(\"spark.mongodb.read.readPreference.name\",\"nearest\") \\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbd66c5a-8732-4c1f-9516-6d51bf35b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Unit Cost: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a DataFrame from a excel file: (Local or HDFS)\n",
    "\n",
    "readPath = \"venkata_tasks/SampleData.xls\"\n",
    "dataAddress = \"'SalesOrders'!A1\"\n",
    "\n",
    "# readPath = \"venkata_tasks/fsi-2006.xlsx\"\n",
    "# dataAddress = \"A1\"\n",
    "\n",
    "df = spark.read \\\n",
    "  .format('excel') \\\n",
    "  .option(\"dataAddress\", dataAddress) \\\n",
    "  .options(**excelReadOpts) \\\n",
    "  .load(readPath)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba4a604b-bab6-4491-87e6-ab767175f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/08 23:35:29 WARN ExcelHeaderChecker: Number of column in Excel header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 0\n",
      "Excel file: file:///Users/sounak/Personal/assignments/venkata_tasks/SampleData.xls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.select(\"country\").orderBy(\"country\").show(10,truncate=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e8552-0aee-4be4-accd-839b40325a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
